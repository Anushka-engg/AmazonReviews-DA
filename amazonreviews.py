# -*- coding: utf-8 -*-
"""AmazonReviews.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sa5qbWiYuo9mVy0LL-x-ckrWGteauXMo
"""

from google.colab import files
files.upload()  # Select kaggle.json file

from google.colab import files
files.upload()

!pip install kaggle

from google.colab import files
files.upload()  # Select kaggle.json

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

from google.colab import files
uploaded = files.upload()  # This will open a file upload prompt

import os
os.listdir()

from pyspark.sql import SparkSession

# Start Spark session
spark = SparkSession.builder.appName("AmazonReviews").getOrCreate()

# Load dataset into a Spark DataFrame
df_spark = spark.read.csv("snap/amazon-fine-food-reviews", header=True, inferSchema=True)
df_spark.show(5)  # Show first 5 rows

import os
os.listdir()

import zipfile

with zipfile.ZipFile("Reviews.csv.zip", "r") as zip_ref:
    zip_ref.extractall()  # Extracts in the same directory

# Check extracted files
os.listdir()

from pyspark.sql import SparkSession

# Step 1: Start a Spark session
spark = SparkSession.builder.appName("AmazonReviews").getOrCreate()

# Step 2: Load the CSV file into a Spark DataFrame
df_spark = spark.read.csv("Reviews.csv", header=True, inferSchema=True)

# Step 3: Show the first 5 rows
df_spark.show(5)

df_spark.printSchema()

df_spark.count()

from pyspark.sql.functions import col, sum
df_spark.select([sum(col(c).isNull().cast("int")).alias(c) for c in df_spark.columns]).show()

df_spark = df_spark.dropna()

df_spark = df_spark.dropDuplicates()

from pyspark.sql.functions import to_date

df_spark = df_spark.withColumn("Review_Date", to_date(col("Time").cast("timestamp")))
df_spark.show(5)

df_spark.groupBy("Score").count().orderBy("Score", ascending=False).show()

df_spark.groupBy("ProductId").count().orderBy("count", ascending=False).show(10)

from pyspark.sql.functions import length, avg

df_spark = df_spark.withColumn("Review_Length", length(col("Text")))
df_spark.select(avg("Review_Length")).show()

from pyspark.sql.functions import when

df_spark = df_spark.withColumn(
    "Sentiment",
    when(df_spark["Score"] >= 4, "Positive")
    .when(df_spark["Score"] == 3, "Neutral")
    .otherwise("Negative")
)

df_spark.groupBy("Sentiment").count().show()

df_spark.write.csv("Cleaned_Amazon_Reviews.csv", header=True)

df_spark.describe().show()

df_spark.groupBy("ProductId").count().orderBy("count", ascending=False).show(10)

df_spark.groupBy("UserId").count().orderBy("count", ascending=False).show(10)

from pyspark.sql.functions import round

df_spark.groupBy("ProductId").agg(round(avg("Score"),2).alias("Avg_Rating")).orderBy("Avg_Rating", ascending=False).show(10)

from pyspark.sql.functions import to_date, year, month

df_spark = df_spark.withColumn("Review_Date", to_date(col("Time").cast("timestamp")))
df_spark = df_spark.withColumn("Year", year(col("Review_Date")))
df_spark = df_spark.withColumn("Month", month(col("Review_Date")))

df_spark.groupBy("Year").count().orderBy("Year").show()

df_spark.groupBy("Year", "Month").count().orderBy("Year", "Month").show()

from pyspark.sql.functions import split, explode

df_words = df_spark.withColumn("word", explode(split(col("Text"), " ")))
df_words.groupBy("word").count().orderBy("count", ascending=False).show(20)

from pyspark.sql.functions import when

df_spark = df_spark.withColumn(
    "Sentiment",
    when(df_spark["Score"] >= 4, "Positive")
    .when(df_spark["Score"] == 3, "Neutral")
    .otherwise("Negative")
)

df_spark.groupBy("Sentiment").count().show()

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when
from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer, VectorAssembler
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

spark = SparkSession.builder.appName("AmazonReviewsML").getOrCreate()

df = spark.read.csv("Reviews.csv", header=True, inferSchema=True)
df.printSchema()
df.show(10)

df = df.select("Text", "Score").dropna()
df = df.withColumn("Score", df["Score"].cast("integer"))

# Convert ratings to Sentiment Categories
df = df.withColumn(
    "Sentiment",
    when(df["Score"] >= 4, "Positive")
    .when(df["Score"] == 3, "Neutral")
    .otherwise("Negative")
)

df.show(5)

df = df.select("Text", "Score").dropna()
df = df.withColumn("Score", df["Score"].cast("integer"))

# Convert ratings to Sentiment Categories
df = df.withColumn(
    "Sentiment",
    when(df["Score"] >= 4, "Positive")
    .when(df["Score"] == 3, "Neutral")
    .otherwise("Negative")
)

df.show(5)

tokenizer = Tokenizer(inputCol="Text", outputCol="words")
df_tokens = tokenizer.transform(df)
df_tokens.show(5)

stopwords_remover = StopWordsRemover(inputCol="words", outputCol="filtered_words")
df_clean = stopwords_remover.transform(df_tokens)
df_clean.select("filtered_words").show(10, truncate=False)

tokenizer = Tokenizer(inputCol="Text", outputCol="words")
df_tokens = tokenizer.transform(df)
df_tokens.show(5)

hashingTF = HashingTF(inputCol="filtered_words", outputCol="raw_features", numFeatures=1000)
df_hashed = hashingTF.transform(df_clean)

idf = IDF(inputCol="raw_features", outputCol="features")
idf_model = idf.fit(df_hashed)
df_features = idf_model.transform(df_hashed)

df_features.select("features").show(5)

train_data, test_data = df_final.randomSplit([0.8, 0.2], seed=42)

lr = LogisticRegression(featuresCol="features", labelCol="Sentiment_Label")
model = lr.fit(train_data)

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when
from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer, VectorAssembler
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# Step 1: Start Spark Session
spark = SparkSession.builder.appName("AmazonReviewsML").getOrCreate()

# Step 2: Load Dataset
df = spark.read.csv("Reviews.csv", header=True, inferSchema=True)

# Step 3: Select & Clean Data
df = df.select("Text", "Score").dropna()
df = df.withColumn("Score", df["Score"].cast("integer"))

# Convert ratings to Sentiment Categories
df = df.withColumn(
    "Sentiment",
    when(df["Score"] >= 4, "Positive")
    .when(df["Score"] == 3, "Neutral")
    .otherwise("Negative")
)

# Step 4: Tokenization
tokenizer = Tokenizer(inputCol="Text", outputCol="words")
df_tokens = tokenizer.transform(df)

# Remove Stopwords
stopwords_remover = StopWordsRemover(inputCol="words", outputCol="filtered_words")
df_clean = stopwords_remover.transform(df_tokens)

# Convert Words to TF-IDF
hashingTF = HashingTF(inputCol="filtered_words", outputCol="raw_features", numFeatures=1000)
df_hashed = hashingTF.transform(df_clean)

idf = IDF(inputCol="raw_features", outputCol="features")
idf_model = idf.fit(df_hashed)
df_features = idf_model.transform(df_hashed)

# Convert Sentiment to Numeric Labels
indexer = StringIndexer(inputCol="Sentiment", outputCol="Sentiment_Label")
df_final = indexer.fit(df_features).transform(df_features)

# Select Required Columns
df_final = df_final.select("features", "Sentiment_Label")
df_final.show(5)

train_data, test_data = df_final.randomSplit([0.8, 0.2], seed=42)

# Train Logistic Regression Model
lr = LogisticRegression(featuresCol="features", labelCol="Sentiment_Label")
model = lr.fit(train_data)

# Make Predictions
predictions = model.transform(test_data)
predictions.select("Sentiment_Label", "prediction").show(10)

# Evaluate Model
evaluator = MulticlassClassificationEvaluator(labelCol="Sentiment_Label", metricName="accuracy")
accuracy = evaluator.evaluate(predictions)
print(f"Model Accuracy: {accuracy:.2f}")

train_data, test_data = df_final.randomSplit([0.8, 0.2], seed=42)

lr = LogisticRegression(featuresCol="features", labelCol="Sentiment_Label")
model = lr.fit(train_data)

predictions = model.transform(test_data)
predictions.select("Sentiment_Label", "prediction").show(10)

evaluator = MulticlassClassificationEvaluator(labelCol="Sentiment_Label", metricName="accuracy")
accuracy = evaluator.evaluate(predictions)
print(f"Model Accuracy: {accuracy:.2f}")

